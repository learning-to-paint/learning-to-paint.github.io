<!DOCTYPE html><html><head>
<title>Unsupervised Doodling and Painting with Improved SPIRAL</title>
<!--Generated by LaTeXML (version 0.8.3) http://dlmf.nist.gov/LaTeXML/.-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link type="text/css" rel="stylesheet" href="https://learning-to-paint.web.app/style.css"><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>
<link rel="icon" href="https://learning-to-paint.web.app/favicon.ico" sizes="16x16 32x32 64x64 128x128" type="image/vnd.microsoft.icon">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Unsupervised Doodling and Painting 
<br class="ltx_break">with Improved SPIRAL</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">John F. J. Mellor 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eunbyung Park 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yaroslav Ganin 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Igor Babuschkin 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tejas Kulkarni 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dan Rosenbaum 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andy Ballard 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Theophane Weber 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Oriol Vinyals 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">S. M. Ali Eslami
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We investigate using reinforcement learning agents as generative models of images <cite class="ltx_cite ltx_citemacro_citep">(Ganin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite>. A <span class="ltx_text ltx_font_italic">generative agent</span> controls a simulated painting environment, and is trained with rewards provided by a discriminator network simultaneously trained to assess the realism of the agent’s samples, either unconditional or reconstructions. Compared to prior work, we make a number of improvements to the architectures of the agents and discriminators that lead to intriguing and at times surprising results. We find that when sufficiently constrained, generative agents can learn to produce images with a degree of visual abstraction, despite having only ever seen real photographs (no human brush strokes). And given enough time with the painting environment, they can produce images with considerable realism. These results show that, under the right circumstances, some aspects of human drawing can emerge from simulated embodiment, without the need for external supervision, imitation or social cues. Finally, we note the framework’s potential for use in creative applications.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering" width="677" height="399" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" style="font-size:90%;">Despite not seeing examples of human drawings, generative agents can produce images with a degree of visual abstraction, in a diverse array of styles, and they can scale to approach realistic results. Agents can also learn to reconstruct, for instance parsing Omniglot characters into strokes.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Through a human’s eyes, the world is much more than just the patterns of photons reflected in our retinas. We have the ability to form high-level interpretations of objects and scenes, and we use these to reason, to build memories, to communicate, to plan and to take actions.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">An important open problem in artificial intelligence is how these interpretations are produced in the absence of labelled datasets to train from. The idea of generative modelling has offered a possible solution, whereby high-level representations are obtained by first training models to generate outputs that resemble real images <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Welling, <a href="#bib.bib18" title="Auto-encoding variational bayes" class="ltx_ref">2013</a>; Gregor<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib19" title="Draw: a recurrent neural network for image generation" class="ltx_ref">2015</a>; Eslami<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib17" title="Neural scene representation and rendering" class="ltx_ref">2018</a>)</cite>. The underlying hypothesis is that if a concise representation is sufficient for the model to reproduce an image, then that representation has, in some form, captured the essence of the contents of that image.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Modern implementations of the generative modeling approach use large, flexible, mostly unstructured neural networks to model the generative process, with state-of-the-art models now achieving photo-realistic results <cite class="ltx_cite ltx_citemacro_citep">(Karras<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib20" title="A style-based generator architecture for generative adversarial networks" class="ltx_ref">2018</a>; Brock<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib3" title="Large scale GAN training for high fidelity natural image synthesis" class="ltx_ref">2019</a>)</cite> even at high resolutions. Whilst immensely impressive, such results raise two intriguing questions: 1. To what extent is photo-realism necessary for learning of high-level descriptions of images? 2. Is it ever beneficial to incorporate grounded <span class="ltx_text ltx_font_italic">knowledge</span> and <span class="ltx_text ltx_font_italic">structure</span> in the generative model?</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Humans have been representing and reconstructing their visual sensations, using physical tools to draw and sculpt depictions of those sensations, for at least 60,000 years <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib8" title="U-Th dating of carbonate crusts reveals Neandertal origin of Iberian cave art" class="ltx_ref">2018</a>)</cite>, well before the development of symbolic writing systems. And it has been hypothesized that abstraction away from raw sensations and use of physical tools are essential components of certain human cognitive abilities <cite class="ltx_cite ltx_citemacro_citep">(Clark and Chalmers, <a href="#bib.bib6" title="The extended mind" class="ltx_ref">1998</a>; Lake<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib5" title="Human-level concept learning through probabilistic program induction" class="ltx_ref">2015</a>, <a href="#bib.bib4" title="Building machines that learn and think like people" class="ltx_ref">2017</a>)</cite>. Within the field of artificial intelligence itself, human figurative drawings have long provided inspiration for the study of meaningful representation of object concepts <cite class="ltx_cite ltx_citemacro_citep">(Minsky and Papert, <a href="#bib.bib7" title="Artificial intelligence progress report" class="ltx_ref">1972</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">We would like to be able to create generative models that similarly use physical grounding. In this work, we equip artificial agents with the same tools that we use to create reconstructions of images (namely digital brushes, pens and spray cans). We train these agents with reinforcement learning to interact with digital painting environments <cite class="ltx_cite ltx_citemacro_citep">(Renold, <a href="#bib.bib34" title="MyPaint" class="ltx_ref">2004</a>; Li, <a href="#bib.bib38" title="Fluid paint" class="ltx_ref">2017</a>)</cite>.
The agents act by placing strokes on a simulated canvas and changing the brush size, pressure and colour as they do so. Building on the work by <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite>, we consider a setting where the agents’ rewards are specified by jointly-trained adversarial discriminator networks. An important aspect of this approach is that the structure is not in the agent itself, but mostly in the environment that it interacts with.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">In summary, we make the following contributions: <span class="ltx_text ltx_font_bold">(a) Quality:</span> We scale up and tune the work of <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite>. We use several simple but general tricks that improve the performance of the framework and show that these changes positively impact the fidelity and realism of generated images. <span class="ltx_text ltx_font_bold">(b) Abstraction:</span> We demonstrate that generative agents can learn to draw recognizable objects with a small number of strokes of the simulated brush and without supervision. These drawings appear to capture the essential elements of objects at a surprisingly high level: for instance generating faces by drawing two eyes, a nose and a mouth each with a single stroke. We also show how the discriminators learn distance metrics that can prioritize semantic likeness over pixel similarity. Our results in this work are primarily qualitative, in part due to the subjective nature of the phenomena being studied, however we provide systematic ablations of the results in the supplementary material.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering" width="675" height="307" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">An architecture for generative agents.<span class="ltx_text ltx_font_medium"> There are two learnable components in the SPIRAL architecture: the policy network or agent <span id="S1.F2.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span> and the discriminator network <span id="S1.F2.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span></span></span></span></span>. The policy network takes as input a partially completed canvas and produces the parameters of the agent’s action. This action is sent to the renderer <span id="S1.F2.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">R</span></span></span></span></span></span></span></span></span> as a command to update the canvas. The discriminator network takes as input the final canvas and attempts to classify it as real or fake. This network is trained using both images from the real dataset and rendered samples from the agent.</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Spiral++</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">In this work, we build closely on the SPIRAL architecture <cite class="ltx_cite ltx_citemacro_citep">(Ganin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite>, see <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;2</span></a>. The goal of SPIRAL is to train a policy <span id="S2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span> that controls a black-box rendering simulator <span id="S2.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">R</span></span></span></span></span></span></span></span></span> using adversarial learning <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib21" title="Generative adversarial nets" class="ltx_ref">2014</a>)</cite>. At every time step, <span id="S2.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span> observes the current state of the drawing and produces a rendering command <span id="S2.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_{t}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span></span></span> used by <span id="S2.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">R</span></span></span></span></span></span></span></span></span> to update the canvas. Each command specifies the appearance of a single brush stroke (<span class="ltx_text ltx_font_italic">e.g.</span>, its shape and colour, the amount of pressure applied to the brush and so on – the action space is discussed in greater detail in <a href="#A1" title="Appendix A Compound action space ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;A</span></a>). In SPIRAL, the learning signal for the policy comes in the form of the reward computed by a discriminator network <span id="S2.p1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span></span></span></span></span> simultaneously optimized to tell apart examples from some target distribution of images <span id="S2.p1.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{d}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span></span></span></span></span> and final renders produced by the agent (denoted as <span id="S2.p1.m8" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{g}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span></span></span></span></span></span></span></span></span>). The outcome of the training procedure is a policy that generates a sequences of commands such that <span id="S2.p1.m9" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{g}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span></span></span></span></span></span></span></span></span> is close to <span id="S2.p1.m10" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{d}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span></span></span></span></span>. Although the SPIRAL framework was shown to perform well on a number of tasks, in its original form it is limited in terms of image fidelity and scalability. We improve these aspects of the framework by introducing
the simple but effective modifications detailed in the following subsections. We refer to the resulting model as SPIRAL++ in order to distinguish it from prior work.
</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Spectral Normalization</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">SPIRAL <cite class="ltx_cite ltx_citemacro_citep">(Ganin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> followed the setup of <cite class="ltx_cite ltx_citemacro_cite">Gulrajani<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib12" title="Improved training of Wasserstein GANs" class="ltx_ref">2017</a>)</cite>, namely that of a WGAN-GP discriminator architecture. In SPIRAL++, we instead revert to the standard GAN loss <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib21" title="Generative adversarial nets" class="ltx_ref">2014</a>)</cite>, but with spectral normalization as described in <cite class="ltx_cite ltx_citemacro_cite">Miyato<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib11" title="Spectral normalization for generative adversarial networks" class="ltx_ref">2018</a>)</cite>. Spectral normalization controls the Lipschitz constant of the discriminator by literally setting the spectral norm of each of its layers to equal 1. The motivation for this is to stabilize the training of the discriminator. In practice we find that spectral normalization significantly improves the fidelity of the generated images, as ablated in <a href="#A6.SS1" title="F.1 Spectral Normalization ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;F.1</span></a> in the appendix.
</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Temporal Credit Assignment</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">SPIRAL’s reward structure (namely that of rewarding the agent only on the last step of each episode) provides agents with the flexibility to learn non-greedy image generation policies, however in practice, the sparsity of the learning signal limits the length of the episodes for which reinforcement learning techniques yield positive results. <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> only report results on episodes of length 20. It would be desirable to maintain this flexibility to achieve non-greedy policies whilst relaxing the difficulty of the learning problem. In SPIRAL++, instead of only providing the agent a reward at the end of the episode based on the final discriminator loss, we calculate the discriminator loss at every step based on the partially-drawn canvas, and at every timestep the agent gets the 1-step improvement in loss as its reward: <span id="S2.SS2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="r_{t}=D(\mathcal{R}(a_{1:t}))-D(\mathcal{R}(a_{1:t-1}))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">R</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">R</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>.
This form of reward redistribution was shown by <cite class="ltx_cite ltx_citemacro_cite">Ng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib15" title="Policy invariance under reward transformations: theory and application to reward shaping" class="ltx_ref">1999</a>)</cite> to lead to the same optimal policies
if the reinforcement learning discount factor <span id="S2.SS2.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span></span></span></span></span> is set to equal 1. Intuitively, when <span id="S2.SS2.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span>, all terms cancel out in the calculation of returns apart from the final step’s reward. However we obtained better results with <span id="S2.SS2.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span></span></span></span></span> between 0 and 0.99, causing the agent to be more greedy but introducing some bias <cite class="ltx_cite ltx_citemacro_citep">(Guez<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib14" title="Learning to search with MCTSnets" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Huang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib25" title="Learning to paint with model-based deep reinforcement learning" class="ltx_ref">2019</a>)</cite> similarly use this technique, concurrent with this work. We provide an ablation of the effectiveness of temporal credit assignment in <a href="#A6.SS2" title="F.2 Temporal Credit Assignment ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;F.2</span></a> in the appendix.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Complement Discriminator</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">When in conditional training mode, the discriminator is used to train agents such that when given a target image <span id="S2.SS3.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{x}_{\mbox{target}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.667em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span></span></span></span></span>, they produce reconstructions <span id="S2.SS3.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{\mathbf{x}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span></span></span></span></span></span></span> that are as similar as possible to the target. Note that conditional agents are trained differently than their unconditional counterparts. <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> achieve this by conditioning the discriminator on the target. Specifically, in each forward pass, the input to the discriminator is either: 1. a <span class="ltx_text ltx_font_italic">fake</span> image pair <span id="S2.SS3.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(\mathbf{x}_{\mbox{target}},{\mathbf{x}})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.667em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-texatom MJXc-space1"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>, or 2. a <span class="ltx_text ltx_font_italic">real</span> image pair <span id="S2.SS3.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(\mathbf{x}_{\mbox{target}},\mathbf{x}_{\mbox{target}})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.667em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.667em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>. Agents are encouraged to move in a direction that makes renderings <span id="S2.SS3.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{\mathbf{x}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span></span></span></span></span></span></span> more similar to the targets <span id="S2.SS3.p1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{x}_{\mbox{target}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.667em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span></span></span></span></span> in order to fool the discriminator. A potential downfall of this method is that the discriminator can in theory find a simple shortcut to achieve perfect performance: it can compare the two images in each input pair, and if they are not identical, pixel to pixel, then it will know that it is a fake image pair. Whilst this strategy allows the discriminator to achieve its training objective, it can lead to a sub-optimal loss surface for the agent.</p>
</div>
<figure id="S2.F3" class="ltx_figure ltx_align_floatright">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="S2.F3.fig1" class="ltx_figure ltx_align_center" style="background-color:#000000;"><img src="im/rtm/13fully_conditioned_pair.png" id="S2.F3.g1" class="ltx_graphics" width="541" height="271" alt="">
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S2.F3.fig2" class="ltx_figure ltx_align_center" style="background-color:#000000;"><img src="im/rtm/13fully_conditioned_pair.png" id="S2.F3.g2" class="ltx_graphics" width="541" height="271" alt="">
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" style="font-size:90%;">Fake input <span id="S2.F3.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(\mathbf{x}_{\mbox{target}}^{1-m},{\mathbf{x}}^{m})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-stack" style="vertical-align: -0.651em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></span></figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p">To overcome this we use what we call a <span class="ltx_text ltx_font_italic">complement discriminator</span>. In each forward pass, a binary image mask <span id="S2.SS3.p2.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{\mathbf{m}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">m</span></span></span></span></span></span></span></span></span></span></span> is sampled. We mask the rendered image by <span id="S2.SS3.p2.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{\mathbf{m}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">m</span></span></span></span></span></span></span></span></span></span></span> to obtain <span id="S2.SS3.p2.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{\mathbf{x}}^{m}={\mathbf{m}}\cdot{\mathbf{x}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">m</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-texatom MJXc-space2"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span></span></span></span></span></span></span>, and mask the target both by <span id="S2.SS3.p2.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{\mathbf{m}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">m</span></span></span></span></span></span></span></span></span></span></span> to obtain <span id="S2.SS3.p2.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{x}_{\mbox{target}}^{m}={\mathbf{m}}\cdot\mathbf{x}_{\mbox{target}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-stack" style="vertical-align: -0.651em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">m</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.667em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span></span></span></span></span> and by the complement of <span id="S2.SS3.p2.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{\mathbf{m}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">m</span></span></span></span></span></span></span></span></span></span></span> to obtain <span id="S2.SS3.p2.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{x}_{\mbox{target}}^{1-m}=(1-{\mathbf{m}})\cdot\mathbf{x}_{\mbox{target}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-stack" style="vertical-align: -0.651em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-texatom MJXc-space2"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">m</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.667em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span></span></span></span></span>. We then define the fake input to the discriminator to be <span id="S2.SS3.p2.m8" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(\mathbf{x}_{\mbox{target}}^{1-m},{\mathbf{x}}^{m})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-stack" style="vertical-align: -0.651em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> and the real input to the discriminator to be <span id="S2.SS3.p2.m9" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(\mathbf{x}_{\mbox{target}}^{1-m},\mathbf{x}_{\mbox{target}}^{m})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-stack" style="vertical-align: -0.651em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">x</span></span></span></span></span><span class="mjx-stack" style="vertical-align: -0.651em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 141.4%;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em;">target</span></span></span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>. In our experiments we mask the left/right half (see <span class="ltx_text ltx_font_italic">e.g.</span> <a href="#S2.F3" title="Figure 3 ‣ 2.3 Complement Discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;3</span></a>) or top/bottom half of the image, however random masks are likely to work just as well <cite class="ltx_cite ltx_citemacro_citep">(Pathak<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib66" title="Context encoders: feature learning by inpainting" class="ltx_ref">2016</a>)</cite>. This trick makes it impossible for the discriminator to detect the real pair by simply comparing pixels, therefore creating a more suitable loss surface for the agent. We ablate the effect of the complement discriminator in <a href="#A6.SS3" title="F.3 Complement discriminator ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;F.3</span></a> in the appendix.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Population discriminator</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p">For best results we dynamically tune learning rate and entropy cost hyper-parameters using population based training (PBT, <cite class="ltx_cite ltx_citemacro_citep">Jaderberg<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib33" title="Population based training of neural networks" class="ltx_ref">2017</a></cite>). However, unlike <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> which also employs this technique, we typically train with a periodically-evolving population of 10 generative agents sharing a <span class="ltx_text ltx_font_italic">single</span> discriminator. The effect of this modification is three-fold. Since the discriminator receives fake samples from all the policy learners, this means that it gets updated more frequently than individual generators allowing us to remove the replay buffer from the original SPIRAL distributed setup. PBT also automatically resurrects individual generators should they collapse during training. Additionally each generator can now specialize in a subset of the modes of the full distribution, such that the population collectively covers the full distribution. Put another way, this technique allows us to increase the relative representational capacity of the generators as compared to the discriminator. We show in the experiments that this setup sometimes leads to the different agents in a single population learning different painting styles. Note, however, that we do not employ any explicit techniques to encourage or guarantee population diversity.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Results</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Owing to the diverse range of settings under which the generative agents framework can be examined (with different action sets, rendering environments, brush types, episode lengths, agent and discriminator hyper-parameters, and so on), we first provide highlights of our main findings in <a href="#S3" title="3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section&nbsp;3</span></a>. In figures, we show selected agents to illustrate phenomena that are <span class="ltx_text ltx_font_italic">possible</span> to observe with the framework. This is not to imply that the settings those agents were trained with are necessary or sufficient for the observed results. In the appendix we provide a systematic analysis of the major components of SPIRAL++ via controlled ablations.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">We show highlights of our experiments on Celeba-HQ <cite class="ltx_cite ltx_citemacro_citep">(Karras<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib35" title="Progressive growing of gans for improved quality, stability, and variation" class="ltx_ref">2017</a>)</cite> in <a href="#S3.F4" title="Figure 4 ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;4</span></a>. Amongst all the framework’s settings, the one we observed to have the most profound impact on the agents’ behaviour was the number of brush strokes (steps) they were allowed in each episode to generate an image. Agents that were constrained with short episodes learned qualitatively different policies than those that could afford numerous interactions, producing images with a degree of visual abstraction, not unlike how humans do when similarly constrained <cite class="ltx_cite ltx_citemacro_citep">(Selim, <a href="#bib.bib57" title="Spiderman 10 min 1 min 10 sec speed challenge" class="ltx_ref">2018</a>; Fan<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib26" title="Pragmatic inference and visual abstraction enable contextual flexibility during visual communication" class="ltx_ref">2019</a>)</cite>. For this reason we structure the results into two sections: short episodes in <a href="#S3.SS1" title="3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;3.1</span></a> and long episodes in <a href="#S3.SS2" title="3.2 Long episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;3.2</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<p class="ltx_p ltx_align_center">(a) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-17_barrr.png" id="S3.F4.g1" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break">(b) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-17_dry_spline.png" id="S3.F4.g2" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break">(c) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-17_dry_bezier.png" id="S3.F4.g3" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break">(d) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-fluid.png" id="S3.F4.g4" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break">(e) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-31.png" id="S3.F4.g5" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break">(f) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-400.png" id="S3.F4.g6" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break">(g) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-999.png" id="S3.F4.g7" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break">(h) <span class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="im/gen_episode_length_abstraction-spiral_baseline.png" id="S3.F4.g8" class="ltx_graphics" width="519" height="65" alt=""></span> 
<br class="ltx_break"></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Artificial portraiture with generative agents.<span class="ltx_text ltx_font_medium"> Random unconditional samples generated by agents trained on CelebA-HQ. Separate agents with varying hyper-parameters were trained to generate samples (a, b, c) in 17 steps with various brushes and action spaces, (d) in 19 steps using an oil paint simulator <cite class="ltx_cite ltx_citemacro_citep">(Li, <a href="#bib.bib38" title="Fluid paint" class="ltx_ref">2017</a>)</cite>, (e) in 31 steps, (f) in 400 steps, (g) in 1000 steps. As a baseline, (h) shows unconditional samples generated in 19 steps by our best reimplementation of SPIRAL (<cite class="ltx_cite ltx_citemacro_citep">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a></cite>, WGAN-GP + single discriminator per population + hyperparameter tweaks). Our improved method scales with episode length from relatively abstract to approach realistic-looking results. See <a href="https://learning-to-paint.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://learning-to-paint.github.io</a> for videos.</span></span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Short episodes</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">We encourage the reader to take a few moments to inspect the samples in <a href="#S3.F4" title="Figure 4 ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;4</span></a>(a-c). Each row was generated by a different agent, differing in the settings of their environments (<span class="ltx_text ltx_font_italic">e.g.</span> brush type, action space, episode length).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">It was surprising for us to see the aesthetically pleasing way in which the agent draws faces: <span class="ltx_text ltx_font_italic">e.g.</span> using a large circle to delineate the outline of the face, dots for each of the eyes, a line for the nose and a line for the mouth. Note that the agent has never been exposed to human <span class="ltx_text ltx_font_italic">drawings</span> of human faces, but only to realistic <span class="ltx_text ltx_font_italic">photographs</span> of human faces. Also note that the agents choose to use bright colours and thin strokes to depict salient elements of faces despite no element of the framework encouraging such behaviour. In all cases, the architecture of the agent is constant, and it is the variation in the characteristics of the agent’s environment that creates the diversity of styles. These results serve as an existence proof for the conjecture that some aspects of human drawings of faces can emerge automatically from a learning framework as simple as that of generative agents (namely agents equipped with brushes working against a discriminator), without the need for supervision, imitation or social cues.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">Unlike in most modern GANs which directly output pixels, in this setting there is a large discrepancy between what the generator <span class="ltx_text ltx_font_italic">can</span> produce, and what it <span class="ltx_text ltx_font_italic">should</span> produce. The brushes are too constrained and there is simply not enough time in the episodes for the generator to be able to produce a completely photo-realistic image. And in our experiments the discriminators can always distinguish between generated and real images with high confidence. Nevertheless we observe that when sufficiently regularised, discriminators can provide sufficient signal for meaningful learning to take place, suggesting that they are still capable of ranking generated images in a useful way. We explore this further in <a href="#S3.F5" title="Figure 5 ‣ 3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;5</span></a>, by training agents on color photos but only with various grayscale brushes.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="S3.F4.sf1" class="ltx_figure ltx_align_center"><img src="im/color_ablation-gray_from_gray.png" id="S3.F4.sf1.g1" class="ltx_graphics" width="541" height="361" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">gray with gray (diverse)</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S3.F4.sf2" class="ltx_figure ltx_align_center"><img src="im/color_ablation-gray_from_color.png" id="S3.F4.sf2.g1" class="ltx_graphics" width="541" height="361" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">color with gray (mode-collapse)</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S3.F4.sf3" class="ltx_figure ltx_align_center"><img src="im/color_ablation-black_from_gray.png" id="S3.F4.sf3.g1" class="ltx_graphics" width="541" height="361" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">gray with black (mode-collapse)</span></figcaption>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Tasked with the impossible, these agents make do.<span class="ltx_text ltx_font_medium"> Samples are selected from three 20 step models trained on CelebA-HQ with modified environments. As a baseline, (a) was tasked with generating grayscale photos using a black brush with variable opacity. (b) was tasked with generating color photos using the same variable opacity black brush. (c) was tasked with generating grayscale photos using an opaque black pen. Models (b) and (c) often manage to draw recognizable faces despite the huge gap between what they <span class="ltx_text ltx_font_italic">can</span> and <span class="ltx_text ltx_font_italic">should</span> produce, however both experience severe mode collapse: each agent in these populations generates minor variations on a single image rather than a full distribution of images.</span></span></figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">It is informative to examine how agents interact with the simulated canvas to produce these images. In <a href="#S3.F6" title="Figure 6 ‣ 3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;6</span></a> we show a number of episodes as sequences. We see that agents can learn to manipulate the location, colour and thickness of the brush with precision, constructing the final images stroke by stroke. It is worth noting that due to RL’s objective of maximizing potentially delayed rewards, agent policies often deviate from being purely greedy, and they often take actions that appear to reduce the quality of the image, especially early in the episode, only for it to be revealed to have been important for the final drawing. We revisit this point in <a href="#S3.SS2" title="3.2 Long episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;3.2</span></a> and <a href="#A5.F17" title="Figure 17 ‣ Appendix E Training curves ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;17</span></a> in the appendix.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="im/gen_11steps_filmstrip029.png" id="S3.F6.g1" class="ltx_graphics ltx_centering" width="541" height="46" alt=""></td>
<td class="ltx_subgraphics"><img src="im/gen_11steps_filmstrip001.png" id="S3.F6.g2" class="ltx_graphics ltx_centering" width="541" height="46" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">10 step canvas sequences show precise control.<span class="ltx_text ltx_font_medium"> Agents interact with the canvas to produce the final image, manipulating the location, colour and thickness of the brush with precision. See <a href="https://learning-to-paint.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://learning-to-paint.github.io</a> for videos. </span></span></figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p class="ltx_p">As described in <a href="#S2.SS4" title="2.4 Population discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.4</span></a>, we use population based training (PBT, <cite class="ltx_cite ltx_citemacro_citep">Jaderberg<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib33" title="Population based training of neural networks" class="ltx_ref">2017</a></cite>). In <a href="#S3.F7" title="Figure 7 ‣ 3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;7</span></a> (and <a href="#A2.F11" title="Figure 11 ‣ Appendix B Sample diversity ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;11</span></a> in the appendix) we show samples from three different agents that were trained as part of the same population. We see that the different generators each specialize in a subset of the modes of the full distribution, each producing images with a perceptibly different style. Note that in all three cases the architecture of the agents, their action spaces, and the settings of their environments were identical, and they differ only in their weights and evolved learning rate and entropy cost. Finally, we show conditional samples in <a href="#S3.F8" title="Figure 8 ‣ 3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;8</span></a>. The agent is able to match the higher level statistics of the target images, and even appears to capture the faces’ smiles. In <a href="#A2.F12" title="Figure 12 ‣ Appendix B Sample diversity ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;12</span></a> in the appendix we visualise the agent’s stochasticity by producing multiple samples for the same target image.</p>
</div>
<figure id="S3.F7" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="S3.F7.fig1" class="ltx_figure ltx_align_center"><img src="im/gen_20steps_samplesheet_1.png" id="S3.F7.g1" class="ltx_graphics" width="541" height="541" alt="">
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S3.F7.fig2" class="ltx_figure ltx_align_center"><img src="im/gen_20steps_samplesheet_2.png" id="S3.F7.g2" class="ltx_graphics" width="541" height="541" alt="">
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S3.F7.fig3" class="ltx_figure ltx_align_center"><img src="im/gen_20steps_samplesheet_3.png" id="S3.F7.g3" class="ltx_graphics" width="541" height="541" alt="">
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Within-population and within-agent diversity (short episodes).<span class="ltx_text ltx_font_medium"> We show three sets of samples for three different agents in the same population, showing both the diversity of samples for each agent, and the diversity of samples across agents in the same population. The first agent attempts more figurative line drawings, whilst the agent on the right uses more realistic shading. We do occasionally observe ‘mode collapse’ where certain samples repeat themselves, for instance in the middle agent though there is still slight variation in the way each image is drawn.
</span></span></figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure"><img src="im/rec_20steps_samplesheet.png" id="S3.F8.g1" class="ltx_graphics ltx_centering" width="541" height="136" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Conditional image generation.<span class="ltx_text ltx_font_medium"> The agent is generally able to match the higher level statistics of the targets (top), and even appears to capture the faces’ smiles.</span></span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Long episodes</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">We found temporal credit assignment (TCA, <a href="#S2.SS2" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.2</span></a>) to be crucial for training agents on long episodes. Without TCA, agents only learn to perform meaningful actions in the last 15 to 60 steps or so of the episode, regardless of the episode length. However, with TCA, agents make full use of episodes of up to 1000 steps, leading in turn to qualitatively different generation policies. We train with discounts of 0.9 or 0.99 and unroll lengths of 20-50 in the experiments that follow. See <a href="#S3.F4" title="Figure 4 ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;4</span></a>(f,g) for samples (as well as <a href="#A2.F11" title="Figure 11 ‣ Appendix B Sample diversity ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;11</span></a> and <a href="#A6.F18" title="Figure 18 ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;18</span></a> in the appendix).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">In <a href="#S3.F9" title="Figure 9 ‣ 3.2 Long episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;9</span></a> we show a visualisation of how images are constructed by TCA agents. It can be seen that agents are successful in making good use of hundreds of timesteps, controlling the brush with precision to form comparatively realistic images. Due to the smaller-than-1 discount factor, agents are incentivised to have somewhat believable intermediate canvases, leading to the formation of the most critical elements of faces (<span class="ltx_text ltx_font_italic">e.g.</span> eyes, nose and mouth) even in the earliest steps of the episode. In unconditional generation, much of the form of the final is determined by the randomness of the first few strokes on the canvas, with the agent doing its best towards the end of the episode to complete the result. In this light, the agent can be seen to be similar in spirit to auto-regressive models such as Pixel-RNNs and Pixel-CNNs <cite class="ltx_cite ltx_citemacro_citep">(van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib60" title="Pixel recurrent neural networks" class="ltx_ref">2016</a>)</cite>, but in stroke-space as opposed to pixel-space.</p>
</div>
<figure id="S3.F9" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="im/gen_200steps_filmstrip_1.png" id="S3.F9.g1" class="ltx_graphics ltx_centering" width="541" height="325" alt=""></td>
<td class="ltx_subgraphics"><img src="im/gen_200steps_filmstrip_2.png" id="S3.F9.g2" class="ltx_graphics ltx_centering" width="541" height="325" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Longer sequences demonstrate higher levels of realism.<span class="ltx_text ltx_font_medium"> SPIRAL++ agents make good use of hundreds of timesteps, controlling the brush with precision. Here we show
logarithmically spaced frames from a 1000 and 500 step agent. See <a href="https://learning-to-paint.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://learning-to-paint.github.io</a> for videos. </span></span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Other datasets and environments</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">We conduct a set of additional experiments to assess the generality of the proposed approach. <a href="#A3" title="Appendix C Evaluation on ImageNet ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;C</span></a> presents results for several ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib37" title="ImageNet Large Scale Visual Recognition Challenge" class="ltx_ref">2015</a>)</cite> classes. In <a href="#A4" title="Appendix D Evaluation on Omniglot ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;D</span></a>, we discuss how a slight modification to the painting environment can lead to meaningful parses of the Omniglot <cite class="ltx_cite ltx_citemacro_citep">(Lake<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib5" title="Human-level concept learning through probabilistic program induction" class="ltx_ref">2015</a>)</cite> characters. Finally, <a href="#A7" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;G</span></a> introduces a new oil paint simulator and shows that our agent is able to learn to control it despite the added complexity of the setting.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">Non-photorealistic rendering emerged as an area of digital art that focuses on enabling a wide variety of artistic styles (in contrast to standard computer graphics which has a focus on photorealism). Stroke based rendering is a particular approach in non-photorealistic rendering that creates images by placing discrete elements such as paint strokes or stipples (see <cite class="ltx_cite ltx_citemacro_citep">Hertzmann, <a href="#bib.bib62" title="A survey of stroke-based rendering" class="ltx_ref">2003</a></cite> for a comprehensive review). A common goal in stroke based rendering is to make the painting look like some other image, whilst limiting the total number of strokes <cite class="ltx_cite ltx_citemacro_citep">(Hertzmann, <a href="#bib.bib62" title="A survey of stroke-based rendering" class="ltx_ref">2003</a>)</cite>. Techniques from computer vision have in some cases been used to control the positions of the strokes <cite class="ltx_cite ltx_citemacro_citep">(Zeng<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib63" title="From image parsing to painterly rendering" class="ltx_ref">2009</a>)</cite>. One attractive property of stroke based rendering is that its interpretations of images can, in theory, be executed in the physical world using real robots <cite class="ltx_cite ltx_citemacro_citep">(Kotani and Tellex, <a href="#bib.bib30" title="Teaching Robots to Draw" class="ltx_ref">2019</a>; El Helou<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib58" title="Mobile robotic painting of texture" class="ltx_ref">2019</a>)</cite>. Closely connected are notions of ‘low-complexity’ art <cite class="ltx_cite ltx_citemacro_citep">(Schmidhuber, <a href="#bib.bib9" title="Low-complexity art" class="ltx_ref">1997</a>)</cite>, which describe an algorithmic theory of beauty and aesthetics based on the principles of information theory. It postulates that amongst several comparable images, the most pleasing one is the one with the shortest description. In almost all cases, the aforementioned techniques operate by optimising, either explicitly or implicitly, a fixed objective function without any learnable parameters. New large-scale sources of data have made it possible to model human behaviour more directly in simple drawing domains <cite class="ltx_cite ltx_citemacro_citep">(Ha and Eck, <a href="#bib.bib51" title="A neural representation of sketch drawings" class="ltx_ref">2017</a>; Zhou<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib46" title="Learning to sketch with deep Q networks and demonstrated strokes" class="ltx_ref">2018</a>; Li<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib22" title="Photo-sketching: inferring contour drawings from images" class="ltx_ref">2019</a>)</cite>, potentially making it possible to capture our internal object function in a data-driven manner.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">Superficially, SPIRAL <cite class="ltx_cite ltx_citemacro_citep">(Ganin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> and SPIRAL++ can be seen as a technique for stroke based, non-photorealistic rendering. Similarly to some modern stroke based rendering techniques, the positions of strokes are determined by a learned system (namely, the agent). Unlike traditional methods however, the objective function that determines the goodness of the output image is learned unsupervised via the adversarial objective. This adversarial objective allows us to train without access to ground-truth strokes, in contrast to <span class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_cite">Ha and Eck (<a href="#bib.bib51" title="A neural representation of sketch drawings" class="ltx_ref">2017</a>); Zhou<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib46" title="Learning to sketch with deep Q networks and demonstrated strokes" class="ltx_ref">2018</a>); Li<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib22" title="Photo-sketching: inferring contour drawings from images" class="ltx_ref">2019</a>)</cite>, enabling the method to be applied to domains where labels are prohibitively expensive to collect, and for it to discover surprising and unexpected styles.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">There are a number of works that use constructions similar to SPIRAL to tune the parameters of non-differentiable simulators <cite class="ltx_cite ltx_citemacro_citep">(Louppe<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib40" title="Adversarial variational optimization of non-differentiable simulators" class="ltx_ref">2019</a>; Ruiz<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib42" title="Learning to simulate" class="ltx_ref">2018</a>; Kar<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib49" title="Meta-sim: learning to generate synthetic datasets" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Frans and Cheng (<a href="#bib.bib29" title="Unsupervised image to sequence translation with canvas-drawer networks" class="ltx_ref">2018</a>); Nakano (<a href="#bib.bib31" title="Neural painters: a learned differentiable constraint for generating brushstroke paintings" class="ltx_ref">2019</a>); Zheng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib44" title="StrokeNet: a neural painting environment" class="ltx_ref">2019</a>); Huang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib25" title="Learning to paint with model-based deep reinforcement learning" class="ltx_ref">2019</a>)</cite> achieve remarkable learning speed by relying on back-propagation through a learned model of the renderer, however they rely on being able to train an accurate model of the environment in the first instance. We further elaborate on this difference in <a href="#A7" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;G</span></a>.
On the applications side, <cite class="ltx_cite ltx_citemacro_cite">El-Nouby<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib27" title="Tell, draw, and repeat: generating and modifying images based on continual linguistic instruction" class="ltx_ref">2018</a>); Lázaro-Gredilla<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib41" title="Beyond imitation: zero-shot task transfer on robots by learning concepts as cognitive programs" class="ltx_ref">2019</a>); Agrawal<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib50" title="Generating diverse programs with instruction conditioned reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> build systems that generate outcomes iteratively, conditioned on ongoing linguistic input or feedback. <cite class="ltx_cite ltx_citemacro_cite">Ellis<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib39" title="Learning to infer graphics programs from hand-drawn images" class="ltx_ref">2018</a>); Liu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib43" title="Learning to describe scenes with programs" class="ltx_ref">2019</a>)</cite> use inference techniques to convert simple scenes into programs expressing abstract regularities. Technical advances such as these (for instance the use of neural simulators and model-based RL) are largely orthogonal to what we introduce in this paper, and are likely to improve the performance of SPIRAL++ further when used in combination. Nonetheless we note that to the best of our knowledge, very few models have the capability to paint images without conditioning on a target image, and even fewer do so at a similar level of abstraction, and lead to as many surprising, emergent styles as those obtained with SPIRAL++.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Conclusions</h2>

<figure id="S5.F10" class="ltx_figure"><img src="im/creative_gen_ns31_barrr_sp.png" id="S5.F10.g1" class="ltx_graphics ltx_centering" width="541" height="338" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" style="font-size:90%;">Selected samples from a single population of agents. The agents find a great variety of ways of working within the constraints of the brush they were given to evoke faces in just 32 steps.</span></figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We showed that when sufficiently constrained, generative agents can learn to produce images with a degree of visual abstraction, despite having never been shown human drawings. To the best of our knowledge, this is the first time that an unsupervised artificial system has discovered visual abstractions of this kind, abstractions that resemble those made by children and novice illustrators. We stress that our aim has been to show that certain results are at least possible to achieve with the generative agents framework, not that the specifics of our setting are necessarily the best way of achieving those results. It is important to acknowledge that our results are mostly qualitative, in part due to the subjective nature of the phenomena being studied (<span class="ltx_text ltx_font_italic">e.g.</span> improved realism and abstraction as compared to SPIRAL). Nevertheless an important line of work will be to quantify these effects via controlled human studies, or even by exposing the generated samples to the markets <cite class="ltx_cite ltx_citemacro_citep">(Cohn, <a href="#bib.bib68" title="AI art at christie’s sells for $432,500" class="ltx_ref">2018</a>)</cite>!</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">The abstraction results arise due to the constraints imposed on the generator by the renderer and the episode’s finite horizon. The generator cannot manipulate pixels directly in the way that generative models typically do; instead it has to obey the physics of the rendering simulation. In other words, the generator is <span class="ltx_text ltx_font_italic">embodied</span>, albeit in a simulation. The discriminator must also be well regularized for this result. It is worth noting that the framework does utilize structure to obtain its results, but this structure is built into the environment, not the agent itself. This stands in contrast with efforts that achieve abstraction by structuring the model or agent, and provides a potentially useful data point for recent debates around the utility of structure (see <span class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_citep">Sutton, <a href="#bib.bib54" title="The bitter lesson" class="ltx_ref">2019</a>; Welling, <a href="#bib.bib53" title="Do we still need models or just more data and compute?" class="ltx_ref">2019</a></cite>).</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">We also showed that such generative agents can produce outputs with considerable realism and complexity when given enough time with the environment. This is a considerable step up from the original SPIRAL, providing encouraging evidence that the approach can be scaled to other sequence generation tasks where expert sequences are difficult to obtain for imitation, for instance in music generation, program synthesis, chemical synthesis and protein folding.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">Finally, we note the framework’s potential for creative and artistic applications. Machine learning techniques such as GANs <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib21" title="Generative adversarial nets" class="ltx_ref">2014</a>)</cite> and neural style transfer <cite class="ltx_cite ltx_citemacro_citep">(Gatys<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib55" title="A neural algorithm of artistic style" class="ltx_ref">2016</a>)</cite> are increasingly being used for creative investigation in the artistic community. Generative agents have an uncommon ability to generate novel aesthetic styles with no human input except for a choice of brush (see <span class="ltx_text ltx_font_italic">e.g.</span> <a href="#S5.F10" title="Figure 10 ‣ 5 Discussion and Conclusions ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;10</span></a>). The extent to which any of these systems can be considered <span class="ltx_text ltx_font_italic">creative</span> in and of themselves is open for discussion <cite class="ltx_cite ltx_citemacro_citep">(Hertzmann, <a href="#bib.bib56" title="Can computers create art?" class="ltx_ref">2018</a>)</cite>, however we look forward to seeing whether the proposed framework can provide new points for consideration in that debate.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We would like to thank Aishwarya Agrawal, Nicolas Heess, Simon Kohl, Adam Kosiorek and David Ha for insightful discussions and comments on this manuscript.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib50" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Agrawal, M. Malinowski, F. Hill, S. M. A. Eslami, O. Vinyals, and T. Kulkarni (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generating diverse programs with instruction conditioned reinforced adversarial learning</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1812.00898</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Brock, J. Donahue, and K. Simonyan (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Large scale GAN training for high fidelity natural image synthesis</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://openreview.net/forum?id=B1xsqj09Fm" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A3.p2" title="Appendix C Evaluation on ImageNet ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Clark and D. Chalmers (1998)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The extended mind</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">analysis</span> <span class="ltx_text ltx_bib_volume">58</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;7–19</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Cohn (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">AI art at christie’s sells for $432,500</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.nytimes.com/2018/10/25/arts/design/ai-art-sold-christies.html" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 Discussion and Conclusions ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. El Helou, S. Mandt, A. Krause, and P. Beardsley (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mobile robotic painting of texture</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">2019 International Conference on Robotics and Automation (ICRA)</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;640–647</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. El-Nouby, S. Sharma, H. Schulz, D. Hjelm, L. El Asri, S. E. Kahou, Y. Bengio, and G. W. Taylor (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Tell, draw, and repeat: generating and modifying images based on continual linguistic instruction</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1811.09845</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Ellis, D. Ritchie, A. Solar-Lezama, and J. Tenenbaum (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to infer graphics programs from hand-drawn images</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;6059–6068</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. M. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka, and K. Gregor (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Neural scene representation and rendering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">360</span> (<span class="ltx_text ltx_bib_number">6394</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;1204–1210</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. E. Fan, R. D. Hawkins, M. Wu, and N. D. Goodman (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pragmatic inference and visual abstraction enable contextual flexibility during visual communication</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Brain &amp; Behavior</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 2522-087X</span>,
<a href="http://dx.doi.org/10.1007/s42113-019-00058-7" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1007/s42113-019-00058-7" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Fernando (2004)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GPU gems: programming techniques, tips, and tricks for real-time graphics</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">590</span>,  <span class="ltx_text ltx_bib_publisher">Addison-Wesley Reading</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A7.p4" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>.
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Frans and C. Cheng (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised image to sequence translation with canvas-drawer networks</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1809.08340</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A7.p1" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>,
<a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Ganin, T. Kulkarni, I. Babuschkin, S. M. A. Eslami, and O. Vinyals (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Synthesizing programs for images using reinforced adversarial learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 35th International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A1.p1" title="Appendix A Compound action space ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>,
<a href="#A6.F18" title="Figure 18 ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure 18</span></a>,
<a href="#A7.p3" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>,
<a href="#A9.p1" title="Appendix I Hyperparameters ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix I</span></a>,
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Unsupervised Doodling and Painting <span class="ltx_text"> </span>with Improved SPIRAL</span></span>,
<a href="#S1.p5" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p6" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS1.p1" title="2.1 Spectral Normalization ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a href="#S2.SS2.p1" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.2</span></a>,
<a href="#S2.SS3.p1" title="2.3 Complement Discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.3</span></a>,
<a href="#S2.SS4.p1" title="2.4 Population discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.4</span></a>,
<a href="#S2.p1" title="2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.F4" title="Figure 4 ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>,
<a href="#S4.p2" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. Gatys, A. Ecker, and M. Bethge (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A neural algorithm of artistic style</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Vision</span> <span class="ltx_text ltx_bib_volume">16</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;326</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1534-7362</span>,
<a href="http://dx.doi.org/10.1167/16.12.326" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1167/16.12.326" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p4" title="5 Discussion and Conclusions ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generative adversarial nets</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in neural information processing systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2672–2680</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p1" title="2.1 Spectral Normalization ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a href="#S2.p1" title="2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S5.p4" title="5 Discussion and Conclusions ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Draw: a recurrent neural network for image generation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICML</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Guez, T. Weber, I. Antonoglou, K. Simonyan, O. Vinyals, D. Wierstra, R. Munos, and D. Silver (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to search with MCTSnets</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the International Conference on Machine Learning (ICML)</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://openreview.net/forum?id=r1TA9ZbA-" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_incollection">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improved training of Wasserstein GANs</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems 30</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p1" title="2.1 Spectral Normalization ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Ha and D. Eck (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A neural representation of sketch drawings</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1704.03477</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S4.p2" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Hertzmann (2003)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A survey of stroke-based rendering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Computer Graphics and Applications</span> <span class="ltx_text ltx_bib_volume">23</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;70–81</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Hertzmann (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Can computers create art?</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Arts</span> <span class="ltx_text ltx_bib_volume">7</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;18</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 2076-0752</span>,
<a href="http://dx.doi.org/10.3390/arts7020018" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3390/arts7020018" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p4" title="5 Discussion and Conclusions ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. L. Hoffmann, C. Standish, M. García-Diez, P. B. Pettitt, J. Milton, J. Zilhão, J. J. Alcolea-González, P. Cantalejo-Duarte, H. Collado, and R. De Balbín (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">U-Th dating of carbonate crusts reveals Neandertal origin of Iberian cave art</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">359</span> (<span class="ltx_text ltx_bib_number">6378</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;912–915</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Z. Huang, W. Heng, and S. Zhou (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to paint with model-based deep reinforcement learning</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1903.04411</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A7.p1" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>,
<a href="#S2.SS2.p1" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.2</span></a>,
<a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, and K. Simonyan (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Population based training of neural networks</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1711.09846</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS4.p1" title="2.4 Population discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.4</span></a>,
<a href="#S3.SS1.p5" title="3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Kar, A. Prakash, M. Liu, E. Cameracci, J. Yuan, M. Rusiniak, D. Acuna, A. Torralba, and S. Fidler (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Meta-sim: learning to generate synthetic datasets</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1904.11621</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Karras, T. Aila, S. Laine, and J. Lehtinen (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Progressive growing of gans for improved quality, stability, and variation</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1710.10196</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Karras, S. Laine, and T. Aila (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A style-based generator architecture for generative adversarial networks</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1812.04948</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. P. Kingma and M. Welling (2013)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Auto-encoding variational bayes</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1312.6114</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. P. Kingma and J. Ba (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Adam: a method for stochastic optimization</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1412.6980</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A9.p1" title="Appendix I Hyperparameters ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix I</span></a>.
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Kotani and S. Tellex (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Teaching Robots to Draw</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE International Conference on Robotics and Automation (ICRA)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Human-level concept learning through probabilistic program induction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">350</span> (<span class="ltx_text ltx_bib_number">6266</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;1332–1338</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.p1" title="Appendix D Evaluation on Omniglot ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix D</span></a>,
<a href="#S1.p4" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS3.p1" title="3.3 Other datasets and environments ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Building machines that learn and think like people</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Behavioral and Brain Sciences</span> <span class="ltx_text ltx_bib_volume">40</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Lázaro-Gredilla, D. Lin, J. S. Guntupalli, and D. George (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Beyond imitation: zero-shot task transfer on robots by learning concepts as cognitive programs</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science Robotics</span> <span class="ltx_text ltx_bib_volume">4</span> (<span class="ltx_text ltx_bib_number">26</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;eaav3150</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 2470-9476</span>,
<a href="http://dx.doi.org/10.1126/scirobotics.aav3150" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1126/scirobotics.aav3150" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Li (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fluid paint</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://david.li/paint/" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A7.p4" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>,
<a href="#S1.p5" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.F4" title="Figure 4 ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Li, Z. Lin, R. Mech, E. Yumer, and D. Ramanan (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Photo-sketching: inferring contour drawings from images</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">WACV 2019: IEEE Winter Conf. on Applications of Computer Vision</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S4.p2" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Liu, Z. D. Wu, D. A. Ritchie, W. T. Freeman, J. B. Tenenbaum, and J. Wu (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to describe scenes with programs</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICLR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Louppe, J. Hermans, and K. Cranmer (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Adversarial variational optimization of non-differentiable simulators</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The 22nd International Conference on Artificial Intelligence and Statistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;1438–1447</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Minsky and S. A. Papert (1972)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Artificial intelligence progress report</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Massachusetts Institute of Technology</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Spectral normalization for generative adversarial networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://openreview.net/forum?id=B1QRgziT-" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p1" title="2.1 Spectral Normalization ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Nakano (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Neural painters: a learned differentiable constraint for generating brushstroke paintings</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1904.08410</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A7.p1" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>,
<a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Y. Ng, D. Harada, and S. J. Russell (1999)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Policy invariance under reward transformations: theory and application to reward shaping</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Context encoders: feature learning by inpainting</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9781467388511</span>,
<a href="http://dx.doi.org/10.1109/CVPR.2016.278" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1109/cvpr.2016.278" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.p2" title="2.3 Complement Discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.3</span></a>.
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Renold (2004)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MyPaint</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://mypaint.org/" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Ruiz, S. Schulter, and M. Chandraker (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to simulate</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1810.02513</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ImageNet Large Scale Visual Recognition Challenge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal of Computer Vision (IJCV)</span> <span class="ltx_text ltx_bib_volume">115</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;211–252</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1007/s11263-015-0816-y" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A3.p1" title="Appendix C Evaluation on ImageNet ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>,
<a href="#S3.SS3.p1" title="3.3 Other datasets and environments ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Schmidhuber (1997)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Low-complexity art</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Leonardo</span> <span class="ltx_text ltx_bib_volume">30</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;97–103</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Selim (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Spiderman 10 min 1 min 10 sec speed challenge</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.youtube.com/watch?v=x9wn633vl_c" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Sutton (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The bitter lesson</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p2" title="5 Discussion and Conclusions ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pixel recurrent neural networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;1747–1756</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p2" title="3.2 Long episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Welling (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Do we still need models or just more data and compute?</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://staff.fnwi.uva.nl/m.welling/wp-content/uploads/Model-versus-Data-AI-1.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p2" title="5 Discussion and Conclusions ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Zeng, M. Zhao, C. Xiong, and S. Zhu (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">From image parsing to painterly rendering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ACM Trans. Graph.</span> <span class="ltx_text ltx_bib_volume">29</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;2:1–2:11</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Zheng, Y. Jiang, and D. Huang (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">StrokeNet: a neural painting environment</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICLR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A7.p1" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>,
<a href="#S4.p3" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Zhou, C. Fang, Z. Wang, J. Yang, B. Kim, Z. Chen, J. Brandt, and D. Terzopoulos (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to sketch with deep Q networks and demonstrated strokes</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1810.05977</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S4.p2" title="4 Related Work ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Compound action space</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite>, a new stroke is deposited onto the canvas at every step of the environment. Consequently, each stroke has a fixed quadratic Bézier form with start coordinates, mid-point coordinates and end coordinates (in addition to thickness and colour). The agent produces new values for the mid-point and end coordinates at every step and selects a thickness and colour for the stroke. The previous step’s end coordinate is used for the current step’s start coordinate. We refer to this interface as the <span class="ltx_text ltx_font_italic">simple</span> interface.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p class="ltx_p">In this work, we also investigate a <span class="ltx_text ltx_font_italic">compound</span> interface, where each stroke is built up over the course of several environment steps. In each step, the agent produces the coordinates of a new control point in the current stroke. In order to terminate a stroke, the agent can execute a discrete action, at which point the stroke is deposited onto the canvas as a cubic spline going through the control points. The thickness and colour of the stroke are determined at the beginning of a sequence of strokes. The compound interface allows the agents to produce smoother, more precise curves, which can sometimes lead to more aesthetically pleasing results. The compound interface is also instrumental in parsing Omniglot characters, as we show in <a href="#A4" title="Appendix D Evaluation on Omniglot ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;D</span></a>.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Sample diversity</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p"><a href="#S3.F7" title="Figure 7 ‣ 3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;7</span></a> and <a href="#A2.F11" title="Figure 11 ‣ Appendix B Sample diversity ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;11</span></a> show diversity of unconditional samples. <a href="#A2.F12" title="Figure 12 ‣ Appendix B Sample diversity ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;12</span></a> shows that there is also considerable diversity in reconstructions of the same target.</p>
</div>
<figure id="A2.F11" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A2.F11.fig1" class="ltx_figure ltx_align_center"><img src="im/gen_200steps_samplesheet_1.png" id="A2.F11.g1" class="ltx_graphics" width="541" height="271" alt="">
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F11.fig2" class="ltx_figure ltx_align_center"><img src="im/gen_200steps_samplesheet_2.png" id="A2.F11.g2" class="ltx_graphics" width="541" height="271" alt="">
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A2.F11.fig3" class="ltx_figure ltx_align_center"><img src="im/gen_200steps_samplesheet_3.png" id="A2.F11.g3" class="ltx_graphics" width="541" height="271" alt="">
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Within-population and within-agent diversity (long episodes).<span class="ltx_text ltx_font_medium"> We show three sets of samples for three different agents in the same population, showing both the diversity of samples for each agent, and the diversity of samples across agents in the same population. Mode collapse occurs less frequently in long episodes. </span></span></figcaption>
</figure>
<figure id="A2.F12" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="im/rec_20steps_diversity_target.png" id="A2.F12.g1" class="ltx_graphics ltx_centering" width="87" height="87" alt=""></td>
<td class="ltx_subgraphics"><img src="im/rec_20steps_diversity_samples.png" id="A2.F12.g2" class="ltx_graphics ltx_centering" width="346" height="87" alt=""></td>
<td class="ltx_subgraphics"><img src="im/rec_20steps_diversity_mean.png" id="A2.F12.g3" class="ltx_graphics ltx_centering" width="87" height="87" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Conditional generation stochasticity.<span class="ltx_text ltx_font_medium"> We visualise an agent’s stochasticity by producing multiple samples for the same target image. Agent stochasticity is due to each action being sampled auto-regressively. Left: Target image. Middle: Multiple samples from the same agent. Right: Mean image produced by averaging 100 samples.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evaluation on ImageNet</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p">We also try the generative agents framework on the ImageNet dataset <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib37" title="ImageNet Large Scale Visual Recognition Challenge" class="ltx_ref">2015</a>)</cite> to see if it generalizes to more varied images that have not been aligned/cropped. However to keep the task simpler we train on individual classes from ImageNet, so only about 1000 images are used for training each agent, again downscaled to 64 <span id="A3.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> 64.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p class="ltx_p">We sometimes instead train using an infinite dataset of samples for single ImageNet classes generated by a pre-trained BigGAN <cite class="ltx_cite ltx_citemacro_citep">(Brock<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib3" title="Large scale GAN training for high fidelity natural image synthesis" class="ltx_ref">2019</a>)</cite> with a truncation threshold of 0.4. This is similar to training on ImageNet directly, but due to the truncation the samples generated are less diverse: objects tend to be centered, face on, with normal appearances and less cluttered backgrounds.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p class="ltx_p">In both cases generative agents are able to reproduce the rough colors, and sometimes as in <a href="#A3.F13" title="Figure 13 ‣ Appendix C Evaluation on ImageNet ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;13</span></a> can extract high-level structure (catamarans have masts, daisies have radial lines, and so on), but results are less reliable than for faces.</p>
</div>
<figure id="A3.F13" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="im/imagenet_classes-catamaran.png" id="A3.F13.g1" class="ltx_graphics ltx_centering" width="173" height="173" alt=""></td>
<td class="ltx_subgraphics"><img src="im/imagenet_classes-daisy.png" id="A3.F13.g2" class="ltx_graphics ltx_centering" width="173" height="173" alt=""></td>
<td class="ltx_subgraphics"><img src="im/imagenet_classes-phone.png" id="A3.F13.g3" class="ltx_graphics ltx_centering" width="173" height="173" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Cherry-picked samples of reconstruction with complement discriminator, trained for 20 steps on single classes of ImageNet.<span class="ltx_text ltx_font_medium"> Left: Trained on ImageNet catamaran class. Middle: Trained on BigGAN samples of daisies. Right: Trained on BigGAN samples of phones.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Evaluation on Omniglot</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p">We also experiment with applying the generative agents framework to the Omniglot dataset <cite class="ltx_cite ltx_citemacro_citep">(Lake<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib5" title="Human-level concept learning through probabilistic program induction" class="ltx_ref">2015</a>)</cite>. In this setting, we train conditional generative agents to reconstruct Omniglot characters, thereby learning a mapping from bitmap representations into strokes. Note that we do not use the human stroke data for training. We primarily use agents with compound action spaces (<a href="#A1" title="Appendix A Compound action space ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;A</span></a>) to allow them to express more complex, intricately curved strokes when parsing. We additionally introduce a number of hyperparameters to encourage the agent to learn more natural behaviours. In particular, the environment rewards the agent with a small penalty for each time a new stroke is created, as well as a penalty on the total length of each stroke. Without these penalties, agents achieve almost perfect reconstructions but with un-natural movements. We set the values of these parameters via trial and error and visual inspection. A sample of these results can be seen in <a href="#A4.F14" title="Figure 14 ‣ Appendix D Evaluation on Omniglot ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;14</span></a> and <a href="#A4.F15" title="Figure 15 ‣ Appendix D Evaluation on Omniglot ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;15</span></a>.</p>
</div>
<figure id="A4.F14" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="im/rec_omniglot_targets.png" id="A4.F14.g1" class="ltx_graphics ltx_centering" width="101" height="761" alt=""></td>
<td class="ltx_subgraphics"><img src="im/rec_omniglot_filmstrips.png" id="A4.F14.g2" class="ltx_graphics ltx_centering" width="1920" height="761" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Omniglot parsing.<span class="ltx_text ltx_font_medium"> Left: Target images. Right: Sequence of actions taken by a single agent to recreate the target images. Red dots signify a request from the agent to commit the current compound stroke to the canvas and to start a new stroke. Green dots indicate locations of commands that build up a compound stroke. The agent is mostly successful in parsing characters into plausible strokes. At times, it fails to complete the character in the allocated episode length.</span></span></figcaption>
</figure>
<figure id="A4.F15" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="x3.png" id="A4.F15.g1" class="ltx_graphics ltx_centering" width="303" height="237" alt=""></td>
<td class="ltx_subgraphics"><img src="x4.png" id="A4.F15.g2" class="ltx_graphics ltx_centering" width="303" height="237" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Within episode ‘improvement’ of image quality (Omniglot).<span class="ltx_text ltx_font_medium"> Left: We plot the L2 distance of the canvas to its target image for 5 different episodes of a 40 step agent (each curve a different episode). Right: The amount of improvement in L2 distance in each step.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Training curves</h2>

<div id="A5.p1" class="ltx_para">
<p class="ltx_p"><a href="#A5.F16" title="Figure 16 ‣ Appendix E Training curves ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;16</span></a>, <a href="#A5.F17" title="Figure 17 ‣ Appendix E Training curves ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;17</span></a> show training curves for representative agents on the CelebA-HQ dataset.</p>
</div>
<figure id="A5.F16" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="x5.png" id="A5.F16.g1" class="ltx_graphics ltx_centering" width="312" height="238" alt=""></td>
<td class="ltx_subgraphics"><img src="x6.png" id="A5.F16.g2" class="ltx_graphics ltx_centering" width="313" height="238" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Conditional training curves with long episodes.<span class="ltx_text ltx_font_medium"> Left: Mean discriminator reward achieved by the generators on the final step of the episode as training progresses. Note that the absolute value of the curves cannot be compared across different agents, due to each competing with their own discriminators. Right: Mean L2 distance of the final generated image and the target image. Note that the agents are trained to maximise the discriminator reward, but we observe some correlation between that objective and minimisation of L2 error.</span></span></figcaption>
</figure>
<figure id="A5.F17" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="x7.png" id="A5.F17.g1" class="ltx_graphics ltx_centering" width="298" height="239" alt=""></td>
<td class="ltx_subgraphics"><img src="x8.png" id="A5.F17.g2" class="ltx_graphics ltx_centering" width="315" height="239" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Within episode ‘improvement’ of image quality (CelebA).<span class="ltx_text ltx_font_medium"> Left: We plot the L2 distance of the canvas to its target image for 5 different episodes of a conditional 200 step agent trained to reconstruct specific target images with discount factor 0.99 (each curve a different episode). Right: The amount of improvement in L2 distance in each step. We see that the agent often takes actions that increase the distance between the canvas and the target.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Ablations</h2>

<div id="A6.p1" class="ltx_para">
<p class="ltx_p">We provide ablations to show the impact of our design choices. <a href="#A6.F18" title="Figure 18 ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;18</span></a> shows the combined improvement from all our modifications detailed under <a href="#S2" title="2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section&nbsp;2</span></a>. The following sections separately ablate important modifications.</p>
</div>
<figure id="A6.F18" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A6.F18.fig1" class="ltx_figure ltx_align_center">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A6.F17.sf1" class="ltx_figure"><img src="im/1804_01118_celeba_rec.png" id="A6.F17.sf1.g1" class="ltx_graphics" width="541" height="406" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">Conditional SPIRAL</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A6.F17.sf2" class="ltx_figure"><img src="im/improvements-conditional.png" id="A6.F17.sf2.g1" class="ltx_graphics" width="541" height="541" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">Conditional SPIRAL++</span></figcaption>
</figure>
</td>
</tr>
</tbody></table>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A6.F17.sf3" class="ltx_figure ltx_align_center">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="im/improvements-unconditional2.png" id="A6.F17.sf3.g1" class="ltx_graphics" width="541" height="541" alt=""></td>
<td class="ltx_subgraphics"><img src="im/improvements-unconditional2.png" id="A6.F17.sf3.g2" class="ltx_graphics" width="541" height="541" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">Unconditional SPIRAL++</span></figcaption>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Quality improvements over SPIRAL.<span class="ltx_text ltx_font_medium">
(a)&nbsp;Reconstructions taken from <cite class="ltx_cite ltx_citemacro_citep">(Ganin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite>. Note that these results are for CelebA (not CelebA-HQ used in the present paper). Unfortunately <cite class="ltx_cite ltx_citemacro_citep">(Ganin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> did not include generation results.
(b)&nbsp;Selected reconstructions with our improvements.
(c)&nbsp;Selected unconditional generation with our improvements.</span></span></figcaption>
</figure>
<section id="A6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">f.1 </span>Spectral Normalization ablation</h3>

<div id="A6.SS1.p1" class="ltx_para">
<p class="ltx_p"><a href="#A6.F19" title="Figure 19 ‣ F.1 Spectral Normalization ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;19</span></a> shows the impact of the discriminator regularization modifications in <a href="#S2.SS1" title="2.1 Spectral Normalization ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.1</span></a>.</p>
</div>
<figure id="A6.F19" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A6.F18.sf1" class="ltx_figure ltx_align_center"><img src="im/sn_ablation-wgangp.png" id="A6.F18.sf1.g1" class="ltx_graphics" width="541" height="541" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">Gradient penalty (WGAN-GP)</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A6.F18.sf2" class="ltx_figure ltx_align_center"><img src="im/rtm/10spectral_norm.png" id="A6.F18.sf2.g1" class="ltx_graphics" width="541" height="541" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">Spectral normalization</span></figcaption>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 19</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison of discriminator regularization on 19 step episodes.<span class="ltx_text ltx_font_medium"> Spectral Normalization lets the discriminator guide the agent into producing finer details like eyes, noses and mouths. These samples are from a single agent in the population; <a href="#S3.F7" title="Figure 7 ‣ 3.1 Short episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;7</span></a> shows how different agents in the same population focus on reproducing different aspects of the target image distribution.</span></span></figcaption>
</figure>
</section>
<section id="A6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">f.2 </span>Temporal Credit Assignment ablation</h3>

<div id="A6.SS2.p1" class="ltx_para">
<p class="ltx_p">As mentioned in <a href="#S2.SS2" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.2</span></a>, agents without TCA often struggle to make good use of long episodes. This can be seen in <a href="#A6.F20" title="Figure 20 ‣ F.2 Temporal Credit Assignment ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;20</span></a>. It happens even if episode length is gradually increased as a curriculum. The impact on sample quality of adding TCA can be seen in <a href="#A6.F21" title="Figure 21 ‣ F.2 Temporal Credit Assignment ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;21</span></a>.</p>
</div>
<figure id="A6.F20" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="im/noca_gen_200steps_filmstrip_every_8_and_a_half_frames_3rows.png" id="A6.F20.g1" class="ltx_graphics ltx_centering" width="541" height="203" alt=""></td>
<td class="ltx_subgraphics"><img src="im/noca_gen_200steps_filmstrip_every_8_and_a_half_frames_3rows.png" id="A6.F20.g2" class="ltx_graphics ltx_centering" width="541" height="203" alt=""></td>
<td class="ltx_subgraphics"><img src="im/noca_gen_200steps_filmstrip_every_8_and_a_half_frames_3rows.png" id="A6.F20.g3" class="ltx_graphics ltx_centering" width="541" height="203" alt=""></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 20</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Agents without TCA often waste actions.<span class="ltx_text ltx_font_medium"> These are 24 equally spaced frames taken from a single episode with 199 steps. The agent wastes its first 156 actions drawing brightly colored strokes that it eventually paints over, finally drawing a rough face in the final 43 steps. By comparison, agents with TCA start drawing immediately and gradually refine the image, as in <a href="#S3.F9" title="Figure 9 ‣ 3.2 Long episodes ‣ 3 Experimental Results ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;9</span></a>.</span></span></figcaption>
</figure>
<figure id="A6.F21" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A6.F20.sf1" class="ltx_figure ltx_align_center"><img src="im/ca_ablation_gen-noca.png" id="A6.F20.sf1.g1" class="ltx_graphics" width="541" height="541" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">No credit assignment</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A6.F20.sf2" class="ltx_figure ltx_align_center"><img src="im/ca_ablation_gen-dis0.png" id="A6.F20.sf2.g1" class="ltx_graphics" width="541" height="541" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">TCA (discount 0.0)</span></figcaption>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 21</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">TCA lets agents make better use of 199 steps.<span class="ltx_text ltx_font_medium"> Random unconditional samples.</span></span></figcaption>
</figure>
<div id="A6.SS2.p2" class="ltx_para">
<p class="ltx_p">Remarkably, we were also able to train good agents with a discount factor of precisely zero, meaning they completely greedily paint one stroke at a time all the way from a blank white canvas to a final image. This suggests that in expectation the discriminator loss monotonically decreases from a blank canvas to a completed image. This works both when training the discriminator to reject intermediate canvases as fakes (in which case there is greater diversity of intermediate canvases the further away you get from a blank canvas) and when only training the discriminator on the final canvas.</p>
</div>
</section>
<section id="A6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">f.3 </span>Complement discriminator ablation</h3>

<div id="A6.SS3.p1" class="ltx_para">
<p class="ltx_p"><a href="#A6.F22" title="Figure 22 ‣ F.3 Complement discriminator ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;22</span></a> and <a href="#A6.F23" title="Figure 23 ‣ F.3 Complement discriminator ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;23</span></a> contrast reconstruction with L2 loss or fully-conditioned discriminator against reconstruction with the Complement Discriminator from <a href="#S2.SS3" title="2.3 Complement Discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.3</span></a>.</p>
</div>
<figure id="A6.F22" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A6.F21.sf1" class="ltx_figure ltx_align_center"><img src="im/rtm/14rec_l2.png" id="A6.F21.sf1.g1" class="ltx_graphics" width="541" height="541" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">L2 loss</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A6.F21.sf2" class="ltx_figure ltx_align_center"><img src="im/rtm/14rec_full.png" id="A6.F21.sf2.g1" class="ltx_graphics ltx_centering" width="541" height="1081" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">Conditional GAN</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A6.F21.sf3" class="ltx_figure ltx_align_center"><img src="im/rtm/14rec_half.png" id="A6.F21.sf3.g1" class="ltx_graphics" width="541" height="541" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">Complement Discriminator</span></figcaption>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 22</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Types of conditioning 20 step.<span class="ltx_text ltx_font_medium"> The Complement Discriminator gives rise to more interesting reconstructions that are semantically similar rather than similar in pixel space, for example mouths are open wider if the person was smiling widely.</span></span></figcaption>
</figure>
<figure id="A6.F23" class="ltx_figure"><img src="im/conditioning_ablation_ca.png" id="A6.F23.g1" class="ltx_graphics ltx_centering" width="541" height="226" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 23</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Types of conditioning 200 step.<span class="ltx_text ltx_font_medium"> Four 200-step agents reconstruct the target images in the top row. The top two agents paint bezier curves and were trained using L2 loss with discount <span id="A6.F23.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span></span></span></span></span> of 0 and 0.9 respectively. The bottom two agents were both trained with Complement Discriminator, and paint bezier and spline curves respectively (see <a href="#A1" title="Appendix A Compound action space ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;A</span></a>).</span></span></figcaption>
</figure>
</section>
<section id="A6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">f.4 </span>Action space ablation</h3>

<div id="A6.SS4.p1" class="ltx_para">
<p class="ltx_p">When generative agents learn to operate human software like painting programs, they have a human-interpretable action space, and you can control the model by changing the actions offered by the environment. <a href="#A6.F24" title="Figure 24 ‣ F.4 Action space ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;24</span></a> and <a href="#A6.F25" title="Figure 25 ‣ F.4 Action space ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;25</span></a> show how retraining the agent with different brushes leads to very different styles.</p>
</div>
<figure id="A6.F24" class="ltx_figure"><img src="im/brush_ablation_1.png" id="A6.F24.g1" class="ltx_graphics ltx_centering" width="541" height="631" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 24</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Training with different brushes leads to very different styles.<span class="ltx_text ltx_font_medium"> Agents with identical architectures perform reconstruction of the same target photos (top row) for 20 steps; only the brush they were trained with varies (except the first row which uses the Fluid Paint renderer, <a href="#A7" title="Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;G</span></a>). Continued on next page.</span></span></figcaption>
</figure>
<figure id="A6.F25" class="ltx_figure"><img src="im/brush_ablation_2.png" id="A6.F25.g1" class="ltx_graphics ltx_centering" width="541" height="631" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 25</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Training with different brushes leads to very different styles.<span class="ltx_text ltx_font_medium"> Agents with identical architectures perform reconstruction of the same target photos (top row) for 20 steps; only the brush they were trained with varies. Continued from previous page.</span></span></figcaption>
</figure>
</section>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Comparison with recent model-based approaches to painting</h2>

<div id="A7.p1" class="ltx_para">
<p class="ltx_p">Recently, several papers <cite class="ltx_cite ltx_citemacro_citep">(Frans and Cheng, <a href="#bib.bib29" title="Unsupervised image to sequence translation with canvas-drawer networks" class="ltx_ref">2018</a>; Nakano, <a href="#bib.bib31" title="Neural painters: a learned differentiable constraint for generating brushstroke paintings" class="ltx_ref">2019</a>; Zheng<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib44" title="StrokeNet: a neural painting environment" class="ltx_ref">2019</a>; Huang<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib25" title="Learning to paint with model-based deep reinforcement learning" class="ltx_ref">2019</a>)</cite> proposed to improve sample complexity and stability of generative agents by replacing the actual rendering simulator with a differentiable neural surrogate. The latter is trained offline to predict how a certain action (usually random) affects the state of the canvas. Although this is a promising avenue for research, we would like to mention several scenarios in which model-free approaches like SPIRAL and SPIRAL++ would be more suitable alternatives.</p>
</div>
<div id="A7.p2" class="ltx_para">
<p class="ltx_p">First, one of the most appealing features of the neural environments is that they allow to train agents by directly backpropagating the gradient coming from the objective of interest (<span class="ltx_text ltx_font_italic">e.g.</span>, reconstruction loss or adversarial generator loss). Unfortunately, this means that one has to stick to continuous actions in order to avoid usage of brittle gradient approximators. Continuous actions may be fine for defining locations (<span class="ltx_text ltx_font_italic">e.g.</span>, the end point in a Bézier curve) but are not appropriate for the situations requiring inherently discrete decisions like whether the agent should lift the brush or add one more control point to a spline. In <a href="#A4" title="Appendix D Evaluation on Omniglot ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;D</span></a>, we show that SPIRAL++ performs reasonably well in this latter case.
</p>
</div>
<div id="A7.p3" class="ltx_para">
<p class="ltx_p">Secondly, the success of the agent training largely depends on the quality of the neural model of the environment. The simulator used in <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> and in the experiments discussed so far is arguably easy to learn since new stokes interact with the existing drawing in a fairly straightforward manner. Environments which are highly stochastic or require handling of object occlusions and lighting might pose a challenge for neural network based environment models.</p>
</div>
<div id="A7.p4" class="ltx_para">
<p class="ltx_p">Lastly, while in principle possible, designing a model for a simulator with complex dynamics may be a non-trivial task. The majority of the recent works relying on neural renderers assume that the simulator state is fully represented by the appearance of the canvas and therefore only consider non-recurrent state transition models. There is no need for such an assumption in the SPIRAL framework. We demonstrate this advantage by training our agent in a new painting environment based on Fluid Paint <cite class="ltx_cite ltx_citemacro_citep">(Li, <a href="#bib.bib38" title="Fluid paint" class="ltx_ref">2017</a>)</cite>. A distinctive feature of this renderer is that under the hood it performs fluid simulation on a grid of cells. The simulation is governed by the Navier-Stokes equations and requires the access to the velocity field of the fluid <cite class="ltx_cite ltx_citemacro_citep">(Fernando, <a href="#bib.bib69" title="GPU gems: programming techniques, tips, and tricks for real-time graphics" class="ltx_ref">2004</a>)</cite> which is not directly observable from the drawing. On top of that, unlike MyPaint, Fluid Paint models the behaviour of the brush bristles moving against the canvas surface. Despite having to deal with an arguably more complex setting, SPIRAL++ managed to not only learn the basic control of tool at hand but also exploit some of its peculiarities in an interesting way (<span class="ltx_text ltx_font_italic">e.g.</span>, use of the brush bristles to imitate hair). Please refer to <a href="#A7.F26" title="Figure 26 ‣ Appendix G Comparison with recent model-based approaches to painting ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;26</span></a> and the second row of <a href="#A6.F24" title="Figure 24 ‣ F.4 Action space ablation ‣ Appendix F Ablations ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;24</span></a> for qualitative results.</p>
</div>
<figure id="A7.F26" class="ltx_figure"><img src="im/fluid-gen19-3x3.png" id="A7.F26.g1" class="ltx_graphics ltx_centering" width="541" height="541" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 26</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Selected unconditional 20-step samples.<span class="ltx_text ltx_font_medium"> Notice how these agent can paint hair in just one or two brush strokes, relying on the paint physics to draw many individual strands of hair.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Additional samples</h2>

<div id="A8.p1" class="ltx_para">
<p class="ltx_p"><a href="#A8.F27" title="Figure 27 ‣ Appendix H Additional samples ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;27</span></a> and <a href="#A8.F28" title="Figure 28 ‣ Appendix H Additional samples ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;28</span></a> show further samples of agents that learned interesting painting styles.</p>
</div>
<figure id="A8.F27" class="ltx_figure"><img src="im/radial_faces_2x8.png" id="A8.F27.g1" class="ltx_graphics ltx_centering" width="541" height="136" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 27</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Radial faces.<span class="ltx_text ltx_font_medium"> This agent learned a rather unusual style, building up faces with arcs originating from a single corner of the image.</span></span></figcaption>
</figure>
<figure id="A8.F28" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="A8.F28.fig1" class="ltx_figure ltx_align_center"><img src="im/warhol_grid.png" id="A8.F28.g1" class="ltx_graphics" width="541" height="541" alt="">
</figure>
</td>
<td class="ltx_subfigure">
<figure id="A8.F28.fig2" class="ltx_figure ltx_align_center"><img src="im/warhol_grid.png" id="A8.F28.g2" class="ltx_graphics" width="541" height="541" alt="">
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 28</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Focus on color.<span class="ltx_text ltx_font_medium"> Agents are occasionally Warholesque in their use of color. Perhaps these are compensating for other agents in the population using drab color palettes?</span></span></figcaption>
</figure>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Hyperparameters</h2>

<div id="A9.p1" class="ltx_para">
<p class="ltx_p">The generator learning rates and RL entropy costs were evolved using population based training (<a href="#S2.SS4" title="2.4 Population discriminator ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.4</span></a>), but we found that bad initial values could cause training to fail to converge. For all our experiments each generator sampled initial values of the learning rate from the range (<span id="A9.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1\times 10^{-5}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span></span></span></span></span>, <span id="A9.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="3\times 10^{-4}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span></span></span></span></span>) and entropy cost from the range (<span id="A9.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2\times 10^{-3}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span></span></span>, <span id="A9.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1\times 10^{-1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span>). As in <cite class="ltx_cite ltx_citemacro_cite">Ganin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Synthesizing programs for images using reinforced adversarial learning" class="ltx_ref">2018</a>)</cite> we trained the discriminator using Adam <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib67" title="Adam: a method for stochastic optimization" class="ltx_ref">2014</a>)</cite> with a learning rate of <span id="A9.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1\times 10^{-4}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span></span></span></span></span>, <span id="A9.p1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\beta_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> set to <span id="A9.p1.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span></span></span></span></span></span> and <span id="A9.p1.m8" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\beta_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> set to <span id="A9.p1.m9" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.999"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.999</span></span></span></span></span></span></span>. For short episodes we used an RL discount factor <span id="A9.p1.m10" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span></span></span></span></span> of 1.0, but for long episodes using temporal credit assignment (<a href="#S2.SS2" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.2</span></a>) we found that using values anywhere between 0 (completely greedy) and 0.99 gave good results, on episodes of length up to even 1000 steps. Most of our experiments use unroll lengths of 20 steps; others use unroll lengths of 50 steps with no noticeable performance differences (in particular, longer unroll lengths do not seem to reduce the need for temporal credit assignment (<a href="#S2.SS2" title="2.2 Temporal Credit Assignment ‣ 2 SPIRAL++ ‣ Unsupervised Doodling and Painting with Improved SPIRAL" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection&nbsp;2.2</span></a>). We used batch sizes of 64 for both generators and discriminators. Brush strokes were rendered on a 256x256 canvas, but to reduce compute costs the canvas and images from the dataset were downsampled to 64x64 before being fed to the generator or discriminator.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>


<script type="text/javascript" src="https://learning-to-paint.web.app/script.js"></script></body></html>